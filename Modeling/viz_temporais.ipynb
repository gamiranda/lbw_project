{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, lit, lpad, substring, to_date, col, when\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRegionState(data):\n",
    "\n",
    "    data = data.withColumn(\"REGIAO\",\n",
    "                            when(col(\"UF\").isin([11, 12, 13, 14, 15, 16, 17]), \"Norte\")\n",
    "                            .when(col(\"UF\").isin([21, 22, 23, 24, 25, 26, 27, 28, 29]), \"Nordeste\")\n",
    "                            .when(col(\"UF\").isin([31, 32, 33, 35]), \"Sudeste\")\n",
    "                            .when(col(\"UF\").isin([41, 42, 43]), \"Sul\")\n",
    "                            .when(col(\"UF\").isin([50, 51, 52, 53]), \"Centro Oeste\")\n",
    "                            .otherwise(\"NA\")  # Default value if none of the conditions match\n",
    "                        ).withColumn(\"ESTADO\",\n",
    "                            when(col(\"UF\") == 12, \"Acre\")\n",
    "                            .when(col(\"UF\") == 27, \"Alagoas\")\n",
    "                            .when(col(\"UF\") == 16, \"Amapá\")\n",
    "                            .when(col(\"UF\") == 13, \"Amazonas\")\n",
    "                            .when(col(\"UF\") == 29, \"Bahia\")\n",
    "                            .when(col(\"UF\") == 23, \"Ceará\")\n",
    "                            .when(col(\"UF\") == 53, \"Distrito Federal\")\n",
    "                            .when(col(\"UF\") == 32, \"Espírito Santo\")\n",
    "                            .when(col(\"UF\") == 52, \"Goiás\")\n",
    "                            .when(col(\"UF\") == 21, \"Maranhão\")\n",
    "                            .when(col(\"UF\") == 51, \"Mato Grosso\")\n",
    "                            .when(col(\"UF\") == 50, \"Mato Grosso do Sul\")\n",
    "                            .when(col(\"UF\") == 31, \"Minas Gerais\")\n",
    "                            .when(col(\"UF\") == 15, \"Pará\")\n",
    "                            .when(col(\"UF\") == 25, \"Paraíba\")\n",
    "                            .when(col(\"UF\") == 41, \"Paraná\")\n",
    "                            .when(col(\"UF\") == 26, \"Pernambuco\")\n",
    "                            .when(col(\"UF\") == 22, \"Piauí\")\n",
    "                            .when(col(\"UF\") == 24, \"Rio Grande do Norte\")\n",
    "                            .when(col(\"UF\") == 43, \"Rio Grande do Sul\")\n",
    "                            .when(col(\"UF\") == 33, \"Rio de Janeiro\")\n",
    "                            .when(col(\"UF\") == 11, \"Rondônia\")\n",
    "                            .when(col(\"UF\") == 14, \"Roraima\")\n",
    "                            .when(col(\"UF\") == 42, \"Santa Catarina\")\n",
    "                            .when(col(\"UF\") == 35, \"São Paulo\")\n",
    "                            .when(col(\"UF\") == 28, \"Sergipe\")\n",
    "                            .when(col(\"UF\") == 17, \"Tocantins\")\n",
    "                            .otherwise(\"NA\")  # Default value if no condition matches\n",
    "                        )\n",
    "\n",
    "    return data\n",
    "\n",
    "def CreateState(data):\n",
    "    \"\"\"\n",
    "    uf_to_state = {12: \"Acre\",\n",
    "                    27: \"Alagoas\",\n",
    "                    16: \"Amapá\",\n",
    "                    13: \"Amazonas\",\n",
    "                    29: \"Bahia\",\n",
    "                    23: \"Ceará\",\n",
    "                    53: \"Distrito Federal\",\n",
    "                    32: \"Espírito Santo\",\n",
    "                    52: \"Goiás\",\n",
    "                    21: \"Maranhão\",\n",
    "                    51: \"Mato Grosso\",\n",
    "                    50: \"Mato Grosso do Sul\",\n",
    "                    31: \"Minas Gerais\",\n",
    "                    15: \"Pará\",\n",
    "                    25: \"Paraíba\",\n",
    "                    41: \"Paraná\",\n",
    "                    26: \"Pernambuco\",\n",
    "                    22: \"Piauí\",\n",
    "                    24: \"Rio Grande do Norte\",\n",
    "                    43: \"Rio Grande do Sul\",\n",
    "                    33: \"Rio de Janeiro\",\n",
    "                    11: \"Rondônia\",\n",
    "                    14: \"Roraima\",\n",
    "                    42: \"Santa Catarina\",\n",
    "                    35: \"São Paulo\",\n",
    "                    28: \"Sergipe\",\n",
    "                    17: \"Tocantins\"\n",
    "                }\n",
    "    # Map the UF codes to states and create the new column\n",
    "    data['ESTADO'] = data['UF'].map(uf_to_state)\n",
    "\n",
    "    \"\"\"\n",
    "    data = data.withColumn(\"ESTADO\",\n",
    "                            when(col(\"UF\") == 12, \"Acre\")\n",
    "                            .when(col(\"UF\") == 27, \"Alagoas\")\n",
    "                            .when(col(\"UF\") == 16, \"Amapá\")\n",
    "                            .when(col(\"UF\") == 13, \"Amazonas\")\n",
    "                            .when(col(\"UF\") == 29, \"Bahia\")\n",
    "                            .when(col(\"UF\") == 23, \"Ceará\")\n",
    "                            .when(col(\"UF\") == 53, \"Distrito Federal\")\n",
    "                            .when(col(\"UF\") == 32, \"Espírito Santo\")\n",
    "                            .when(col(\"UF\") == 52, \"Goiás\")\n",
    "                            .when(col(\"UF\") == 21, \"Maranhão\")\n",
    "                            .when(col(\"UF\") == 51, \"Mato Grosso\")\n",
    "                            .when(col(\"UF\") == 50, \"Mato Grosso do Sul\")\n",
    "                            .when(col(\"UF\") == 31, \"Minas Gerais\")\n",
    "                            .when(col(\"UF\") == 15, \"Pará\")\n",
    "                            .when(col(\"UF\") == 25, \"Paraíba\")\n",
    "                            .when(col(\"UF\") == 41, \"Paraná\")\n",
    "                            .when(col(\"UF\") == 26, \"Pernambuco\")\n",
    "                            .when(col(\"UF\") == 22, \"Piauí\")\n",
    "                            .when(col(\"UF\") == 24, \"Rio Grande do Norte\")\n",
    "                            .when(col(\"UF\") == 43, \"Rio Grande do Sul\")\n",
    "                            .when(col(\"UF\") == 33, \"Rio de Janeiro\")\n",
    "                            .when(col(\"UF\") == 11, \"Rondônia\")\n",
    "                            .when(col(\"UF\") == 14, \"Roraima\")\n",
    "                            .when(col(\"UF\") == 42, \"Santa Catarina\")\n",
    "                            .when(col(\"UF\") == 35, \"São Paulo\")\n",
    "                            .when(col(\"UF\") == 28, \"Sergipe\")\n",
    "                            .when(col(\"UF\") == 17, \"Tocantins\")\n",
    "                            .otherwise(\"NA\")  # Default value if no condition matches\n",
    "                        )\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def read_csv_from_api(spark, api_url):\n",
    "    \"\"\"\n",
    "    Optimized method to read CSV data from an API link using PySpark\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Initialized Spark session\n",
    "        api_url (str): URL of the CSV file to be read\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: DataFrame containing the CSV data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Direct streaming of API content\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Create a temporary local file\n",
    "        with open('temp_api_data.csv', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Read the CSV file into a Spark DataFrame\n",
    "        df = spark.read.csv(\n",
    "            'temp_api_data.csv',\n",
    "            header=True,  # Assumes the first row is a header\n",
    "            inferSchema=True,  # Automatically detect column types\n",
    "            sep=';'\n",
    "        )\n",
    "\n",
    "        # Optimized date transformation using built-in Spark functions\n",
    "        df = df.withColumn(\"DTNASC_padded\",\n",
    "                           lpad(col(\"DTNASC\"), 8, '0')).select(\n",
    "            \"IDADEMAE\", \n",
    "            \"RACACORMAE\",\n",
    "            \"CODMUNNASC\",\n",
    "            \"PESO\",\n",
    "            # Optimized date transformation\n",
    "            to_date(\n",
    "                concat(\n",
    "                    lpad(substring(col(\"DTNASC_padded\"), 1, 2), 2, '0'),\n",
    "                    lpad(substring(col(\"DTNASC_padded\"), 3, 2), 2, '0'),\n",
    "                    substring(col(\"DTNASC_padded\"), 5, 4)\n",
    "                ),\n",
    "                'ddMMyyyy'\n",
    "            ).alias(\"DTNASC_padded\")\n",
    "        ).withColumn(\"UF\", substring(col(\"CODMUNNASC\"), 1, 2))\n",
    "\n",
    "        df = createRegion(df)\n",
    "        df = CreateState(df)\n",
    "\n",
    "        return df.drop(\"CODMUNNASC\",\"UF\")\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data from API: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"CSV API Reader - LBW\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2011.csv\n",
      "https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2012.csv\n",
      "https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2013.csv\n",
      "https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2014.csv\n",
      "https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2015.csv\n",
      "https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2016.csv\n",
      "https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2017.csv\n",
      "https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2018.csv\n",
      "https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2019.csv\n",
      "https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2020.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/DNOPEN23.csv\n",
    "https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/DNOPEN22.csv\n",
    "https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/SINASC_2021.csv\n",
    "https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/SINASC_2020.csv\n",
    "https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/SINASC_2019.csv\n",
    "https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/SINASC_2019.csv\n",
    "https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/SINASC_2017.csv\n",
    "https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/SINASC_2016.csv\n",
    "\"\"\"\n",
    "\n",
    "url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2013.csv'\n",
    "\n",
    "dt = read_csv_from_api(spark = spark, api_url = url)\n",
    "\n",
    "for ano in range(11, 21, 1):\n",
    "  url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_20' + str(ano) + '.csv'\n",
    "  print(url)\n",
    "  dt_aux = read_csv_from_api(spark = spark, api_url = url)\n",
    "  time.sleep(10)\n",
    "\n",
    "  dt = dt.union(dt_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt_10 = read_csv_from_api(spark = spark, api_url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2010.csv')\n",
    "time.sleep(10)\n",
    "dt_11 = read_csv_from_api(spark = spark, api_url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2011.csv')\n",
    "time.sleep(10)\n",
    "dt_12 = read_csv_from_api(spark = spark, api_url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2012.csv')\n",
    "time.sleep(10)\n",
    "dt_13 = read_csv_from_api(spark = spark, api_url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2013.csv')\n",
    "time.sleep(10)\n",
    "dt_14 = read_csv_from_api(spark = spark, api_url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2014.csv')\n",
    "time.sleep(10)\n",
    "dt_15 = read_csv_from_api(spark = spark, api_url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2015.csv')\n",
    "time.sleep(10)\n",
    "dt_16 = read_csv_from_api(spark = spark, api_url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2016.csv')\n",
    "time.sleep(10)\n",
    "dt_17 = read_csv_from_api(spark = spark, api_url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2017.csv')\n",
    "time.sleep(10)\n",
    "dt_18 = read_csv_from_api(spark = spark, api_url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2018.csv')\n",
    "time.sleep(10)\n",
    "dt_19 = read_csv_from_api(spark = spark, api_url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2019.csv')\n",
    "time.sleep(10)\n",
    "dt_20 = read_csv_from_api(spark = spark, api_url = 'https://diaad.s3.sa-east-1.amazonaws.com/sinasc/SINASC_2020.csv')\n",
    "time.sleep(10)\n",
    "dt_21 = read_csv_from_api(spark = spark, api_url = 'https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/SINASC_2021.csv')\n",
    "time.sleep(10)\n",
    "dt_22 = read_csv_from_api(spark = spark, api_url = 'https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/DNOPEN22.csv')\n",
    "time.sleep(10)\n",
    "dt_23 = read_csv_from_api(spark = spark, api_url = 'https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/DNOPEN23.csv')\n",
    "time.sleep(10)\n",
    "dt_24 = read_csv_from_api(spark = spark, api_url = 'https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/DNOPEN24.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
