{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_2720\\2903196563.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\", temperature = 0)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama \n",
    "llm = Ollama(model=\"mistral\", temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" LangGraph is a large-scale graph-based language model developed by the Microsoft Research team. It's designed to understand and generate human-like text, but unlike traditional models that process sequences of words in a linear manner, LangGraph uses a graph structure to capture complex dependencies between words more effectively. This allows it to better handle long-range dependencies, ambiguities, and other challenging aspects of natural language processing.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nLangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph\\n\\nBy LangChain\\n7 min read\\nJan 17, 2024\\n\\n\\n\\n\\n\\nTL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we\\'ve implemented already. We will then highlight a few of the common modifications to these runtimes we\\'ve heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We\\'ve invested heavily in the functionality for this with LangChain Expression Language. However, so far we\\'ve lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for handling of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by specifying them as graphs.FunctionalityAt it\\'s core, LangGraph exposes a pretty narrow interface on top of LangChain.StateGraphStateGraph is a class that represents the graph. You initialize this class by passing in a state definition. This state definition represents a central state object that is updated over time. This state is updated by nodes in the graph, which return operations to attributes of this state (in the form of a key-value store).The attributes of this state can be updated in two ways. First, an attribute could be overridden completely. This is useful if you want to nodes to return the new value of an attribute. Second, an attribute could be updated by adding to its value. This is useful if an attribute is a list of actions taken (or something similar) and you want nodes to return new actions taken (and have those automatically added to the attribute).You specify whether an attribute should be overridden or added to when creating the initial state definition. See an example in pseudocode below.from langgraph.graph import StateGraph\\nfrom typing import TypedDict, List, Annotated\\nimport Operator\\n\\n\\nclass State(TypedDict):\\n    input: str\\n    all_actions: Annotated[List[str], operator.add]\\n\\n\\ngraph = StateGraph(State)NodesAfter creating a StateGraph, you then add nodes with graph.add_node(name, value) syntax. The name parameter should be a string that we will use to refer to the node when adding edges. The value parameter should be either a function or LCEL runnable that will be called. This function/LCEL should accept a dictionary in the same form as the State object as input, and output a dictionary with keys of the State object to update.See an example in pseudocode below.graph.add_node(\"model\", model)\\ngraph.add_node(\"tools\", tool_executor)There is also a special END node that is used to represent the end of the graph. It is important that your cycles be able to end eventually!from langgraph.graph import ENDEdgesAfter adding nodes, you can then add edges to create the graph. There are a few types of edges.The Starting EdgeThis is the edge that connects the start of the graph to a particular node. This will make it so that that node is the first one called when input is passed to the graph. Pseudocode for that is:graph.set_entry_point(\"model\")Normal EdgesThese are edges where one node should ALWAYS be called after another. An example of this may be in the basic agent runtime, where we always want the model to be called after we call a tool.graph.add_edge(\"tools\", \"model\")Conditional EdgesThese are where a function (often powered by an LLM) is used to determine which node to go to first. To create this edge, you need to pass in three things:The upstream node: the output of this node will be looked at to determine what to do nextA function: this will be called to determine which node to call next. It should return a stringA mapping: this mapping will be used to map the output of the function in (2) to another node. The keys should be possible values that the function in (2) could return. The values should be names of nodes to go to if that value is returned.An example of this could be that after a model is called we either exit the graph and return to the user, or we call a tool - depending on what a user decides! See an example in pseudocode below:graph.add_conditional_edge(\\n    \"model\",\\n    should_continue,\\n    {\\n        \"end\": END,\\n        \"continue\": \"tools\"\\n    }\\n)CompileAfter we define our graph, we can compile it into a runnable! This simply takes the graph definition we\\'ve created so far an returns a runnable. This runnable exposes all the same method as LangChain runnables (.invoke, .stream, .astream_log, etc) allowing it to be called in the same manner as a chain.app = graph.compile()Agent ExecutorWe\\'ve recreated the canonical LangChain AgentExecutor with LangGraph. This will allow you to use existing LangChain agents, but allow you to more easily modify the internals of the AgentExecutor. The state of this graph by default contains concepts that should be familiar to you if you\\'ve used LangChain agents: input, chat_history, intermediate_steps (and agent_outcome to represent the most recent agent outcome)from typing import TypedDict, Annotated, List, Union\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.messages import BaseMessage\\nimport operator\\n\\n\\nclass AgentState(TypedDict):\\n   input: str\\n   chat_history: list[BaseMessage]\\n   agent_outcome: Union[AgentAction, AgentFinish, None]\\n   intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add] See this notebook for how to get startedChat Agent ExecutorOne common trend we\\'ve seen is that more and more models are \"chat\" models which operate on a list of messages. This models are often the ones equipped with things like function calling, which make agent-like experiences much more feasible. When working with these types of models, it is often intuitive to represent the state of an agent as a list of messages.As such, we\\'ve created an agent runtime that works with this state. The input is a list of messages, and nodes just simply add to this list of messages over time.from typing import TypedDict, Annotated, Sequence\\nimport operator\\nfrom langchain_core.messages import BaseMessage\\n\\n\\nclass AgentState(TypedDict):\\n    messages: Annotated[Sequence[BaseMessage], operator.add] See this notebook for how to get startedModificationsOne of the big benefits of LangGraph is that it exposes the logic of AgentExecutor in a far more natural and modifiable way. We\\'ve provided a few examples of modifications that we\\'ve heard requests for:Force Calling a ToolFor when you always want to make an agent call a tool first. For Agent Executor and Chat Agent Executor.Human-in-the-loopHow to add a human-in-the-loop step before calling tools. For Agent Executor and Chat Agent Executor.Managing Agent StepsFor adding custom logic on how to handle the intermediate steps an agent might take (useful for when there\\'s a lot of steps). For Agent Executor and Chat Agent Executor.Returning Output in a Specific FormatHow to make the agent return output in a specific format using function calling. Only for Chat Agent Executor.Dynamically Returning the Output of a Tool DirectlySometimes you may want to return the output of a tool directly. We provide an easy way to do with the return_direct parameter in LangChain. However, this makes it so that the output of a tool is ALWAYS returned directly. Sometimes, you may want to let the LLM choose whether to return the response directly or not. Only for Chat Agent Executor.Future WorkWe\\'re incredibly excited about the possibility of LangGraph enabling more custom and powerful agent runtimes. Some of the things we are looking to implement in the near future:More advanced agent runtimes from academia (LLM Compiler, plan-and-solve, etc)Stateful tools (allowing tools to modify some state)More controlled human-in-the-loop workflowsMulti-agent workflowsIf any of these resonate with you, please feel free to add an example notebook in the LangGraph repo, or reach out to us at hello@langchain.dev for more involved collaboration!\\n\\n\\nTags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain State of AI 2024 Report\\n\\n\\nBy LangChain\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroducing OpenTelemetry support for LangSmith\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEasier evaluations with LangSmith SDK v0.2\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph Platform: New deployment options for scalable agent infrastructure\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFew-shot prompting to improve tool-calling performance\\n\\n\\nBy LangChain\\n8 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImproving core tool interfaces and docs in LangChain\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            Â© LangChain Blog 2025\\n        \\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "O peso do recém-nascido é um fator importante para sua saúde e seu desenvolvimento. Um individuo que nasce com peso abaixo do esperado pode representar riscos tanto para si quanto para a mãe. Baixo peso ao nascer está relacionado a problemas no desenvolvimento da linguagem. Conseguir prever o risco de baixo peso pode ser uma ferramenta médica importante. Trabalhos anteriores com o mesmo objetivo utilizaram tanto informações da mãe como informações do bebê, a partir de exames neonatal. Este trabalho propõe desenvolver um modelo de Machine Learning para prever o o risco de baixo peso ao nascer a partir de dados demográficos da mãe e do pai, disponíveis no SINASC DATASUS.\n",
    "\n",
    "Segundo a Organização Mundial da Saúde (OMS), Baixo Peso ao Nascer (BPN) é definido como peso inferior a 2500 gramas ao nascer independente do idade gestacional . Quando comparados com bebês que nasceram acima dessa linha de corte, nascidos de baixo peso possuem maior risco de óbito, podem apresentar problemas de crescimento e desenvolvimento intelectual. E quando adultos, podem apresentar maior risco de obesidade, desenvolvimento de doenças do coração e diabetes . Parto prematuro, má nutrição materna, consumo de alcool e cigarro durante a gestação são fatores já conhecidos que aumentam o risco de BPN. \n",
    "\n",
    "A partir do momento que o médico possui a informação de que determinada gravidez possui alto risco de gerar um bebê com baixo peso, uma série de medidas podem ser tomadas para evitar esse desfecho. Melhorar o status nutricional da mãe e o acesso a acompanhamento pré-natal são medidas que tendem a diminuir o risco de Baixo Peso ao Nascer .\n",
    "\n",
    "Além disso, existe uma série de intervenções, em diferentes níveis, para previnir BPN. A nível nacional/regional podem ser adotados programas de proteção social, melhoria na qualidade de água, etc. Já a nível pessoa, a suplementação de nutrientes como ferro e ácido fólico contribuem para um melhor desenvolvimento da gravidez\n",
    "\n",
    "A proporção de BPN no mundo varia, em média, entre cerca de 7% e 17%, com algumas regiões chegando até 26% . A partir de dados obtidos no DATASUS SINASC, em 2022 cerca de 9% dos nascidos vivos possuiam menos de 2500 gramas ao nascer. Conseguir prever com atencedência esse risco pode ser uma importante ferramenta para auxiliar o médico na condução daquela gestação.\n",
    "\n",
    "Trabalhos anteriores anteriores se ocuparam em desenvolver modelos de Machine Learning (ML) utilizando dados demográficos, condições de saúde pré-existentes e informações de exames pré-natal. Nesse artigo nós buscamos desenvolver um modelo de Machine Learning com o objetivo de calcular a probabilidade de Baixo Peso ao Nascer utilizando as variáveis disponíveis n'O Sistema de Informações sobre Nascidos Vivos (SINASC).\n",
    "\n",
    "Diversos autores se ocuparam tanto em identificar fatores de risco quanto em desenvolver modelos de textit{machine learning para prever baixo peso ao nascer. Chen et al. (2013), a partir de uma base de  101.163 partos em 14 diferentes regiões na China pôde identificar que idade da mãe menor que 20 anos, com baixo nível de educação e histórico de partos adversos são fatores associados ao risco de baixo peso ao nascer.\n",
    "\n",
    "Ranjbar et al. (2023) testou o uso dos algoritmos Regressão Logística (RL), Árvore de Decisão, Floresta Aleatória, textit{Deep Learning Fastforward, Light GBM, XGBoost, SVM e KNN com dados da textit{Iranian Maternal and Neonatal Network (IMaN Net) de janeiro de 2020 até janeiro de 2022 para desenvolver um modelo de ML capaz de prever baixo peso ao nascer. Em um universo de 8.853 partos, 1.280 (14,5%) foram de bebês com o desfecho de interesse. Além disso, os pesquisadores dispunham tanto de dados demográficos da mãe quanto sua informações de saúde e comorbidades. Para o treino e validação do modelo, a base foi separada em amostra treino (70%) e teste (30%). Já a validação interna foi realiza com textit{k-fold cross-validation. Com esses dados, a equipe constatou que o XGBoost foi o melhor modelo para a tarefa, com acurácia de 0,79, precisão de 0,87, recall de 0,69 e F1 textit{score de 0,77. E, dentre as variáveis disponíveis, idade gestacional e histórico de BPN foram os principais preditores. \n",
    "\n",
    "Já Islam Pollob et al. (2022) trabalhou com uma base de 2351 respondentes proveniente da textit{Bangladesh Demographic and Health Survey entre 2017 e 2018 para, também desenvolver um modelo capaz de prever BPN nesse contexto. Dentre as observações, 16.2% foram bebês com baixo peso ao nascer. O estudo dispunha de informações demográficas da mãe, do parto ee do bebê. Os algoritmos Regressão Logística e Árvore de Decisão foram utilizados, e observou-se que a RL atingiu os melhores resultados, com acurácia de 0,87 e AUC de 0,59. No entanto, a conclusão do estudo foi de que seria necessária uma solução mais eficiente para ser utilizada na realidade de Bangladesh. Além disso, esse estudo utilizou variáveis obtidas no momento do parto, como local e tipo de parto.\n",
    "\n",
    "Singh et al. (2013) realizou um estudo num hospital universitário de cuidados terciários no norte da Índia, em uma base de  250 BPN e 250 com peso maior ou igual a 2.500g. Além disso, esse estudo teve acesso a informações de partos anteriores e de saúde da mãe, como ganho de peso durante a gravidez, incidência de anemia e má nutrição, nível educacional e etc. Os pesquisadores utilizaram seis variáveis preditoras, sendo elas: ganho de peso inadequado na mãe, quantidade de proteína inadequada na dieta da mãe, parto prematuro anterior, parto BPN anterior, anemia e fumante passiva. Modelando com uma regressão logística, a equipe atingiu, na base de validação, sensibilidade de 72% e especificidade de 64%.\n",
    "\n",
    "Diferente dos trabalhos anteriores, esse estudo busca desenvolver um modelo capaz de indicar o risco de baixo peso ao nascer com base em informações pessoais e demográficas da mãe e do pai. Desse modo, na primeira consulta com o médico já seria possível entender a necessidade de cuidados especiais. Além disso, o Sistema de Informações sobre Nascidos Vivos (SINASC) possui um amplo registro dos partos no Brasil, o que nos permite trabalhar com uma base de dados muito maior do que os trabalhos anteriores.\n",
    "\n",
    "As descobertas desse estudo foram baseadas no Sistema de Informações sobre Nascidos Vivos (SINASC). Este programa foi implementado em 1990 com o objetivo de coletar dados sobre os nascimentos em todo território nacional e para todos os níveis do Sistema de Saúde. Agrupados por ano, as bases disponibilizadas contemplam uma série de informações demográficas sobre a mãe e o pai, além de informações sobre o próprio nascido. Foram selecionados os períodos de 2022 e 2023 para comporem a base de treino e o período de 2024 para a base de teste. Até a presente data, a base correspondente a 2024 possui nascimentos registrados até o dia 11 de junho de 2024.\n",
    "\n",
    "A população alvo foi divida conforme o seu peso, nascidos com menos de 2500g foram chamados Baixo Peso ao Nascer e nascidos com peso registrado maior ou igual a 2500g, Não Baixo Peso ao Nascer. Observações sem o peso registrado foram desconsiderados da base de dados. Além disso, as variáveis correspondentes a número de filhos vivos, número de gestações, número de partos normais, número de partos cesária e número de perdas fetais/abortos foram truncadas em 10, visto que uma proporção infima dos dados apresentaram valores maiores do que esse.\n",
    "\n",
    "Para desenvolver o modelo preditor, os seguintes algorítmos foram testados na tarefa de prever Baixo Peso ao Nascer: eXtreme Gradient Boosting (XGB), Light Gradient Boosting (LGBM), CatBoost, Naive Bayes e Random Forest. Para selecionar quais variáveis seriam consideradas no estudos como preditoras, duas linhas foram seguidas: variáveis sem ligação direta com o bebê, e o parto, e variáveis comuns nos estudos citados na seção de Revisão da Literatura. Desse modo, foram mantidas como possíveis variáveis dependentes a idade da mãe, idade do pai, estado civil da mãe, escolaridade em anos de estudo concluídos da mãe, número de filhos vivos e perdas fetais/abortos, número de partos normais e cesáreos, cor da mãe e tipo de gravidez (Vide Tabela \\ref{tab:tabela_predvars). Todas as variáveis selecionadas possuiam valores faltantes, que foram substituídos pelo valor mais frequente correspondente. Com exceção das variáveis ESTCIVMAE e ESCMAE, que foram substituídas por 9, visto que é um valor padrão para dados faltantes da própria base. Além disso, as variáveis categóricas foram transformadas em variáveis dummy segundo as suas categorias, com a técnica de OneHotEncoder. A linguagem de programação escolhida para desenvolver o estudo foi o Python e as bibliotecas Scikit-learn, Pandas e Seaborn foi utilizada para gerar os modelos de Machine Learning e as análises sobre as variáveis.\n",
    "\n",
    "Apesar dos dados estarem desbalanceados, optou-se por não aplicar nenhuma técnica de balanceamento. Segundo, técnicas de balanceamento de dados não melhoram a capacidade preditiva de modelos estado da arte, como os algoritmos de boosting.  mostrou que balanceamento não melhora a capacidade preditiva de modelos Estado da Arte, como os modelos de Boosting. A recomendação é otimizar o threshold para melhorar modelos desbalanceados, o que foi aplicado neste artigo.\n",
    "\n",
    "A base de dados correspondente aos períodos de 2022 e 2023 foi utilizada tanto para treino quanto para validação interna, em uma proporção de 70% e 30%, respectivamente. A partir dessas amostras foram selecionados os melhores hiperparametros pela biblioteca Optuna, escolhidas as variáveis preditoras finais e escolhido o melhor modelo para a tarefa de predizer o risco de Baixo Peso ao Nascer. Já a base de teste, dados disponíveis de 2024, foi utilizada para reportar as métricas finais de validação do modelo.\n",
    "\n",
    "Já com relação as métricas de validação observadas, este trabalho reporta a matriz de confusão, acurácia, Precision e Recall. A matriz de confusão é uma ferramenta de auxilio para melhor entender a distribuição de acertos e erros do modelo. A acurácia calcula a proporção de acertos do modelo na base de dados. Dado o alto nível de desbalanceamento da variável dependente, esta métrica deve ser observada com cuidado para não levar a interpretações incorretas da capacidade preditiva do modelo. Precision mede a proporção de verdadeiros positivos. Recall mede a assertividade dentre as observações positivas. Em outras palavras, quando a classe verdadeira era positiva, com que frequência o modelo predisse como positiva. Entendendo que o custo de falsos negativos seja maior do que o de falsos positivos para esse estudo, a métrica Recall foi priorizada no treino do modelo.\n",
    "\n",
    "A base de treino do modelo possuia 9,5% de Baixo Peso ao Nascer, dentre 4.074.684 milhões de nascidos entre 2022 e 2023. As Tabelas possuem uma série de informações sobre a amostra com relação ao desfecho estudado.\n",
    "\n",
    "A Tabela mostra que as médias de idade dos pais e das mães são muito similares entre os BPN e Não-BPN. No entanto, a Tabela mostra que mães com até 16 anos ou com mais de 35 anos apresentam maior proporção do desfecho. Assim como com relação a idade do pai, nos casos com até 16 anos ou maior que 45 anos.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12339"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter= RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "997\n"
     ]
    }
   ],
   "source": [
    "print(len(all_splits))\n",
    "print(len(all_splits[1].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding = OllamaEmbeddings(\n",
    "    model=\"mistral\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = all_splits,\n",
    "    embedding = embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'language': 'en', 'source': 'https://blog.langchain.dev/langgraph/', 'start_index': 11170, 'title': 'LangGraph'}, page_content='some state)More controlled human-in-the-loop workflowsMulti-agent workflowsIf any of these resonate with you, please feel free to add an example notebook in the LangGraph repo, or reach out to us at hello@langchain.dev for more involved collaboration!'),\n",
       " Document(metadata={'language': 'en', 'source': 'https://blog.langchain.dev/langgraph/', 'start_index': 3, 'title': 'LangGraph'}, page_content='LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph\\n\\nBy LangChain\\n7 min read\\nJan 17, 2024'),\n",
       " Document(metadata={'language': 'en', 'source': 'https://blog.langchain.dev/langgraph/', 'start_index': 5385, 'title': 'LangGraph'}, page_content='class State(TypedDict):\\n    input: str\\n    all_actions: Annotated[List[str], operator.add]')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\n",
    "    \"what is LangGraph?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub \n",
    "\n",
    "prompt= hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LangGraph is a platform developed by LangChain, designed for creating and managing multi-agent workflows. It allows for more controlled human-in-the-loop workflows and can be found in the LangChain repo. For further collaboration or details, you can reach out to [hello@langchain.dev](mailto:hello@langchain.dev).'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is LangGraph?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
